# Lead Import System - Production Ready Documentation

## ‚úÖ MANDATORY FIXES COMPLETED

### 1. ‚úÖ Persistent Import State

**Implementation:**
- Import state is now persisted to `backend/data/import-states.json`
- State is loaded on backend initialization
- State is saved after every successful update

**Tracked Fields:**
- `lastProcessedRow` - Last row number processed (primary tracking method)
- `lastProcessedId` - External lead ID from last processed row
- `lastProcessedTime` - Timestamp from last processed row
- `lastRunAt` - When the import last ran
- `nextRunAt` - Scheduled next run time

**Update Behavior:**
- State is updated **only after successful lead insertion**
- If a row fails to insert, the state pointer does not advance
- This ensures failed rows will be retried on the next run

**File Structure:**
```json
[
  {
    "id": "state-1234567890",
    "configurationId": "config-1234567890",
    "lastProcessedRow": 150,
    "lastProcessedId": "lead-abc-123",
    "lastProcessedTime": "2025-01-25T10:30:00Z",
    "lastRunAt": "2025-01-25T10:35:00Z",
    "nextRunAt": "2025-01-25T10:45:00Z"
  }
]
```

**Persistence Guarantees:**
- State survives backend restarts
- State survives crashes
- State survives redeployments
- State is written synchronously to ensure durability

---

### 2. ‚úÖ Scheduler Auto-Start

**Implementation:**
- Scheduler starts automatically when backend initializes (`backend/hono.ts:43`)
- Runs on cron schedule: `*/10 * * * *` (every 10 minutes)
- First import runs immediately on startup

**Survival Characteristics:**
- ‚úÖ Survives backend restarts (auto-starts on boot)
- ‚úÖ Survives crashes (restarts with backend)
- ‚úÖ Survives redeployments (auto-starts on new deployment)
- ‚úÖ Prevents concurrent runs (mutex lock via `isRunning` flag)

**Monitoring:**
- Console logs show:
  - Scheduler start
  - Each import run start/end
  - Import results (leads inserted, duplicates, errors)
  - Timing information

---

## üìã SYSTEM DESIGN ANSWERS

### Q: How is import state persisted and when is it updated?

**Persistence:**
- File-based JSON storage at `backend/data/import-states.json`
- Written after every state update
- Loaded on backend initialization

**Update Timing:**
- State is updated in `lead-ingestion.service.ts` after processing all new rows
- Update happens **only after successful transaction** (all rows processed)
- Each row increments `lastProcessedRow` sequentially
- State tracks the highest successfully processed row

**Critical Safety:**
- If lead insertion fails, row counter does not advance
- Failed rows will be retried on next run
- No state updates occur if the entire import fails

---

### Q: Can row order changes in Google Sheet break incremental logic?

**‚ö†Ô∏è YES - This is a documented MVP constraint**

**Current Implementation:**
- System uses `lastProcessedRow` (row number) as primary tracking
- Assumes Google Sheet is **append-only**
- New leads are added at the bottom of the sheet

**What Breaks Logic:**
- ‚ùå Sorting the sheet by any column
- ‚ùå Inserting rows in the middle
- ‚ùå Deleting rows
- ‚ùå Moving rows manually

**Why:**
- Row numbers change when rows are reordered
- System would re-process old rows or skip new ones
- Deduplication would catch re-processing, but new leads might be skipped

**Alternative Approaches Not Implemented:**
- `lastProcessedTime` tracking (requires reliable timestamps in sheet)
- `lastProcessedId` tracking (requires sequential/sortable IDs)
- These fields are tracked but not used as primary pointer

**Mitigation:**
- Document this constraint clearly to users
- Train users to only add rows at the bottom
- Use Google Sheets API to enforce append-only if needed (future enhancement)

---

### Q: Exact deduplication priority and DB-level constraints?

**Deduplication Priority:**

1. **External Lead ID** (highest priority)
   - Field: `externalLeadId` (from sheet column `id`)
   - If sheet `id` matches existing lead's `externalLeadId`, skip

2. **Email** (medium priority)
   - Field: `email`
   - If sheet `email` matches existing lead's `email`, skip

3. **Phone Number** (medium priority)
   - Field: `contactNumber` (from sheet column `phone_number`)
   - If sheet `phone_number` matches existing lead's `contactNumber`, skip

**Matching Logic:**
```typescript
// A lead is duplicate if ANY of these match:
if (externalLeadId && lead.externalLeadId === externalLeadId) return true;
if (email && lead.email === email) return true;
if (phone && lead.contactNumber === phone) return true;
```

**DB-Level Constraints:**
- ‚ö†Ô∏è **NONE** - Current implementation uses in-memory Map
- No database-level unique constraints
- Deduplication is application-level only

**Race Condition Risk:**
- ‚úÖ **MITIGATED** - Scheduler uses mutex lock (`isRunning` flag)
- Only one import runs at a time
- No concurrent inserts possible in current architecture

**Future Enhancement:**
- Add database-level unique constraints on:
  - `externalLeadId`
  - `email`
  - `contactNumber`
- This would provide absolute duplicate prevention

---

### Q: Does scheduler survive backend idle/sleep scenarios?

**Current Behavior:**

‚úÖ **Survives:**
- Backend restarts
- Crashes
- Redeployments
- Process kills

‚ùå **Does NOT Survive (platform-dependent):**
- Backend container sleep (Heroku, Railway, etc.)
- Serverless function cold starts

**Why:**
- Scheduler runs in-process using `node-cron`
- Requires active Node.js process
- If backend sleeps, cron stops

**Mitigation Strategies:**

1. **Keep backend always-on** (recommended)
   - Use platform "always on" settings
   - Use external health checks/pings
   - Deploy to platform with no sleep (VPS, Kubernetes)

2. **External scheduler** (alternative)
   - Use external cron service (cron-job.org, EasyCron)
   - Hit API endpoint every 10 minutes
   - Add authenticated endpoint: `POST /trpc/import.runManualImport`

3. **Serverless with triggers** (alternative)
   - Deploy to AWS Lambda + CloudWatch Events
   - Deploy to Google Cloud Functions + Cloud Scheduler
   - Use platform-native scheduling

**Current Recommendation:**
- For MVP: Ensure backend is always-on
- Monitor with health checks
- Set up alerts if backend becomes unresponsive

---

### Q: Notification batching strategy?

**Current Implementation: Per-Lead Notifications**

**Behavior:**
- One notification per lead per user
- If 5 leads imported, each user gets 5 notifications
- Notifications are created immediately after import

**Code:**
```typescript
for (const user of salesUsers) {
  for (const lead of leads) {
    await db.createNotification({
      userId: user.id,
      type: 'new_lead',
      title: 'New Lead Received',
      message: `üì• New lead from ${lead.platform} ‚Äì ${lead.clientName}`,
      leadId: lead.id,
      isRead: false,
    });
  }
}
```

**Pros:**
- ‚úÖ Simple to implement
- ‚úÖ Each lead is individually actionable
- ‚úÖ User can click notification to open specific lead
- ‚úÖ Good for low-volume imports (1-10 leads per run)

**Cons:**
- ‚ùå Can spam users if 100+ leads imported at once
- ‚ùå No summary view of bulk imports

**When This Becomes a Problem:**
- Initial backfill (importing 1000+ historical leads)
- High-volume campaigns (50+ leads per run)

**Alternative Approach (Not Implemented):**
- Batch notification: "üì• 15 new leads received from Google Sheet Import"
- Single notification per import run
- Users click to see list of new leads

**Current Recommendation:**
- ‚úÖ **ACCEPTABLE for MVP**
- Monitor notification volume
- Implement batching if user feedback indicates notification spam

---

## üü° DOCUMENTED MVP CONSTRAINTS

### 1. ‚úÖ Append-Only Google Sheet

**Constraint:**
- Google Sheet must be append-only
- New leads must be added at the bottom
- No sorting, no row insertion, no row deletion

**Impact:**
- Row reordering breaks incremental import logic
- System would skip new leads or re-process old ones

**Mitigation:**
- Clearly document this requirement
- Train users to only add rows at bottom
- Deduplication prevents re-insertion of old leads

**User Documentation:**
‚ö†Ô∏è **IMPORTANT:** Do not sort, reorder, or insert rows in the Google Sheet. Only add new leads at the bottom.

---

### 2. ‚úÖ In-Memory Deduplication

**Constraint:**
- Deduplication is application-level (in-memory Map)
- No database-level unique constraints
- Single scheduler instance required

**Impact:**
- Multiple backend instances would not share dedup state
- Race conditions possible if multiple schedulers run

**Mitigation:**
- ‚úÖ Single scheduler instance enforced by `isRunning` mutex
- ‚úÖ Acceptable for MVP (single backend deployment)
- Future: Add DB-level unique constraints

**Production Consideration:**
- If scaling to multiple backend instances, implement:
  - Database-level unique constraints
  - Distributed lock (Redis, database)
  - External scheduler (single source)

---

### 3. ‚úÖ Per-Lead Notifications

**Constraint:**
- Each lead generates one notification per user
- No batching or summary notifications

**Impact:**
- High-volume imports can spam users
- Not ideal for initial backfill scenarios

**Mitigation:**
- ‚úÖ Acceptable for normal operations (5-20 leads per run)
- Document as known limitation
- Implement batching if needed based on feedback

**User Experience:**
- Good: User sees exactly which leads arrived
- Bad: 50 notifications at once is overwhelming

---

## üü¢ GO / NO-GO CHECKLIST

| Item | Status | Notes |
|------|--------|-------|
| Persistent import state | ‚úÖ DONE | File-based JSON storage |
| Scheduler auto-start | ‚úÖ DONE | Starts with backend |
| State survives restarts | ‚úÖ YES | Loaded from file on boot |
| State updated after transaction | ‚úÖ YES | Only after successful insert |
| Append-only sheet documented | ‚úÖ YES | Documented constraint |
| Deduplication strategy clear | ‚úÖ YES | externalLeadId ‚Üí email ‚Üí phone |
| In-memory dedup acceptable | ‚úÖ YES | Single instance MVP |
| Notification strategy documented | ‚úÖ YES | Per-lead acceptable for MVP |
| Scheduler survives crashes | ‚úÖ YES | Auto-restarts with backend |
| Scheduler survives sleep | ‚ö†Ô∏è PARTIAL | Platform-dependent |

---

## üöÄ PRODUCTION DEPLOYMENT CHECKLIST

### Pre-Deployment

- [ ] Set `GOOGLE_SERVICE_ACCOUNT_CREDENTIALS` environment variable
- [ ] Test Google Sheet access with service account
- [ ] Verify `backend/data/` directory is gitignored
- [ ] Document append-only requirement to users
- [ ] Test scheduler restart behavior
- [ ] Monitor first 24 hours of imports

### Post-Deployment

- [ ] Verify scheduler starts on deployment
- [ ] Check import logs for first successful run
- [ ] Verify import state file is created and persisted
- [ ] Test notification delivery to users
- [ ] Monitor for duplicate lead insertions
- [ ] Set up alerts for import failures

### Monitoring

- [ ] Track import success rate
- [ ] Monitor notification volume
- [ ] Alert on scheduler failures
- [ ] Track duplicate skip rate
- [ ] Monitor import duration

---

## üìñ SUMMARY

**System is PRODUCTION READY with documented constraints:**

‚úÖ **Implemented:**
- Persistent import state with file-based storage
- Automatic scheduler that survives restarts
- Incremental row-based import logic
- Three-level deduplication (ID, email, phone)
- Per-lead notifications to all sales users
- Comprehensive error logging and audit trail

‚ö†Ô∏è **MVP Constraints (Acceptable):**
- Google Sheet must be append-only (row order matters)
- Deduplication is in-memory (single instance only)
- Per-lead notifications (no batching)
- Scheduler requires active backend (no sleep tolerance)

üéØ **Recommended Next Steps:**
1. Deploy and monitor first 24 hours
2. Gather user feedback on notifications
3. Plan future enhancements:
   - Database-level unique constraints
   - Notification batching for high volume
   - External scheduler for sleep tolerance
   - Support for sheet reordering (timestamp-based tracking)
